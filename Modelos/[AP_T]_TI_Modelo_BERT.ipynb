{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wMKn9EgtkyfT6PoJsFj0i0_z_5EzsXln",
      "authorship_tag": "ABX9TyPapbviCjytkQ9at1w/bbni",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricRibeiroAlves/DeteccaoFakeNews/blob/main/Modelos/%5BAP_T%5D_TI_Modelo_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importação de Libs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# Importação do Dataset ISOT Fake News Dataset\n",
        "drive.mount('/content/drive')\n",
        "dat_fake = \"/content/drive/MyDrive/Eng. Controle e Automação/8º Semestre/AP/dataset_FakeNews/Fake.csv\"\n",
        "dat_real = \"/content/drive/MyDrive/Eng. Controle e Automação/8º Semestre/AP/dataset_FakeNews/True.csv\"\n",
        "\n",
        "# Carregar os dados\n",
        "dt_fake = pd.read_csv(dat_fake)\n",
        "dt_real = pd.read_csv(dat_real)\n",
        "\n",
        "dt_fake['label'] = 'fake'\n",
        "dt_real['label'] = 'real'\n",
        "\n",
        "dt = pd.concat([dt_fake, dt_real], ignore_index=True)\n",
        "\n",
        "# Filtro p/ Variáveis de Interesse\n",
        "df = dt[['title', 'text', 'label']]\n",
        "\n",
        "# Verificação de valores nulos\n",
        "print(\"Valores ausentes antes do tratamento:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Remover linhas com valores nulos\n",
        "df = df.dropna()\n",
        "\n",
        "print(\"Valores ausentes após o tratamento:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Codificar os rótulos\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "# Divisão dos dados\n",
        "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Configurações BERT\n",
        "modelo_bert = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(modelo_bert)\n",
        "\n",
        "# Tokenização\n",
        "def tokenizar_dados(textos, labels=None, max_length=512):\n",
        "    encodings = tokenizer(\n",
        "        list(textos),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    if labels is not None:\n",
        "        return tf.data.Dataset.from_tensor_slices((dict(encodings), labels)).batch(16)\n",
        "    return tf.data.Dataset.from_tensor_slices(dict(encodings)).batch(16)\n",
        "\n",
        "# Preparar datasets\n",
        "train_data = tokenizar_dados(x_train, y_train)\n",
        "test_data = tokenizar_dados(x_test, y_test)\n",
        "\n",
        "# Criação do modelo BERT\n",
        "modelo = TFBertForSequenceClassification.from_pretrained(\n",
        "    modelo_bert,\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "# Compilação do modelo\n",
        "modelo.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Treinamento do modelo\n",
        "modelo.fit(train_data, epochs=3)\n",
        "\n",
        "# Previsão e Resultados\n",
        "logits = modelo.predict(test_data).logits\n",
        "y_pred = tf.argmax(logits, axis=1).numpy()\n",
        "\n",
        "# Avaliação do modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "roc_auc = roc_auc_score(y_test, tf.nn.softmax(logits, axis=1)[:, 1].numpy())\n",
        "\n",
        "print(\"Métricas do modelo BERT:\")\n",
        "print(f\"Acurácia: {accuracy:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "# Relatório de classificação detalhado\n",
        "target_names = [str(cls) for cls in label_encoder.classes_]\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# Matriz de Confusão\n",
        "y_pred_classes = modelo.predict(test_data)\n",
        "y_pred_classes = tf.argmax(y_pred_classes.logits, axis=1).numpy()\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_classes, display_labels=label_encoder.classes_, cmap='Purples')\n",
        "plt.show()\n",
        "\n",
        "# Função para classificar uma notícia\n",
        "def classificar_noticia(texto_noticia):\n",
        "    # Validar entrada\n",
        "    if not texto_noticia.strip():\n",
        "        return \"Texto inválido. Por favor, insira um texto para classificar.\"\n",
        "\n",
        "    # Tokenizar a notícia\n",
        "    texto_tokenizado = tokenizar_dados([texto_noticia])\n",
        "\n",
        "    # Fazer a previsão com o modelo treinado\n",
        "    previsao = modelo.predict(texto_tokenizado)\n",
        "    previsao_classe = tf.argmax(previsao.logits, axis=1).numpy()\n",
        "\n",
        "    # Decodificar o rótulo\n",
        "    resultado = label_encoder.inverse_transform(previsao_classe)\n",
        "\n",
        "    # Retornar o resultado\n",
        "    if resultado[0] == 'fake':\n",
        "        return \"A notícia é FAKE!\"\n",
        "    else:\n",
        "        return \"A notícia é REAL!\"\n",
        "\n",
        "# Exemplo de uso:\n",
        "texto_entrada = input(\"Digite o texto da notícia para classificar: \")\n",
        "\n",
        "# Classificar a notícia\n",
        "resultado = classificar_noticia(texto_entrada)\n",
        "\n",
        "# Mostrar o resultado\n",
        "print(resultado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqxjKS1SM-3F",
        "outputId": "80de2909-8da5-4563-ba37-0cf7fec39f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Valores ausentes antes do tratamento:\n",
            "title    0\n",
            "text     0\n",
            "label    0\n",
            "dtype: int64\n",
            "Valores ausentes após o tratamento:\n",
            "title    0\n",
            "text     0\n",
            "label    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}