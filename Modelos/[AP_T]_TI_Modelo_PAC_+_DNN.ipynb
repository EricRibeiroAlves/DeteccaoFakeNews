{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM06vF4gDECr8TM9gfSAu+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricRibeiroAlves/DeteccaoFakeNews/blob/main/Modelos/%5BAP_T%5D_TI_Modelo_PAC_%2B_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frfsQHUEpcGi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "\n",
        "# Carregar os dados\n",
        "dat_fake = \"/content/drive/MyDrive/Eng. Controle e Automação/8º Semestre/AP/dataset_FakeNews/Fake.csv\"\n",
        "dat_real = \"/content/drive/MyDrive/Eng. Controle e Automação/8º Semestre/AP/dataset_FakeNews/True.csv\"\n",
        "dt_fake = pd.read_csv(dat_fake)\n",
        "dt_real = pd.read_csv(dat_real)\n",
        "\n",
        "dt_fake['label'] = 0  # Fake news -> 0\n",
        "dt_real['label'] = 1  # Real news -> 1\n",
        "\n",
        "dt = pd.concat([dt_fake, dt_real], ignore_index=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TddYu_xod4b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir os dados em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    dt['text'], dt.label, test_size=0.2, random_state=7\n",
        ")\n",
        "\n",
        "# Transformar os textos em vetores TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Treinar o PassiveAggressiveClassifier\n",
        "pac = PassiveAggressiveClassifier(max_iter=50)\n",
        "pac.fit(tfidf_train, y_train)\n",
        "\n",
        "# Predições do PAC no conjunto de treino e teste\n",
        "pac_train_preds = pac.decision_function(tfidf_train)  # Margem de confiança\n",
        "pac_test_preds = pac.decision_function(tfidf_test)\n",
        "\n",
        "# Combinar as predições do PAC com os vetores TF-IDF\n",
        "train_combined = np.hstack((tfidf_train.toarray(), pac_train_preds.reshape(-1, 1)))\n",
        "test_combined = np.hstack((tfidf_test.toarray(), pac_test_preds.reshape(-1, 1)))\n"
      ],
      "metadata": {
        "id": "cz_b4qfbhwKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar o modelo de Deep Learning\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(train_combined.shape[1],)),  # Entrada combinada\n",
        "    Dropout(0.3),  # Prevenção de overfitting\n",
        "    Dense(64, activation='relu'),  # Camada intermediária\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),  # Camada adicional\n",
        "    Dense(1, activation='sigmoid')  # Saída binária\n",
        "])\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Treinar o modelo\n",
        "history = model.fit(train_combined, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2,\n",
        "                    verbose=1)\n",
        "\n",
        "# Avaliar no conjunto de teste\n",
        "loss, accuracy = model.evaluate(test_combined, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "avqmxPqjhpvI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}